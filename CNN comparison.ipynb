{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from CSV files\n",
    "n = ['id', 'date','name','text','typr','rep','rtw','faw','stcount','foll','frien','listcount']\n",
    "data_positive = pd.read_csv('positive.csv', sep=';',error_bad_lines=False, names=n, usecols=['text'])\n",
    "data_negative = pd.read_csv('negative.csv', sep=';',error_bad_lines=False, names=n, usecols=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create balanced dataset\n",
    "sample_size = 10000\n",
    "raw_data = np.concatenate((data_positive['text'].values[:sample_size], \n",
    "                           data_negative['text'].values[:sample_size]), axis=0) \n",
    "labels = [1]*sample_size + [0]*sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "def preprocess_text(text):\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', text)\n",
    "    text = re.sub('@[^\\s]+','USER', text)\n",
    "    text = text.lower().replace(\"ё\", \"е\")\n",
    "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
    "    text = re.sub(' +',' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "data = [preprocess_text(t) for t in raw_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "SENTENCE_LENGTH = 26\n",
    "NUM = 50000\n",
    "\n",
    "def get_sequences(tokenizer, x):\n",
    "    sequences = tokenizer.texts_to_sequences(x)\n",
    "    return pad_sequences(sequences, maxlen=SENTENCE_LENGTH)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train_seq = get_sequences(tokenizer, x_train)\n",
    "x_test_seq = get_sequences(tokenizer, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Загружаем обученную модель\n",
    "w2v_model = Word2Vec.load('tweets_model.w2v')\n",
    "DIM = w2v_model.vector_size \n",
    "# Инициализируем матрицу embedding слоя нулями\n",
    "embedding_matrix = np.zeros((NUM, DIM))\n",
    "# Добавляем NUM=100000 наиболее часто встречающихся слов из обучающей выборки в embedding слой\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= NUM:\n",
    "        break\n",
    "    if word in w2v_model.wv.vocab.keys():\n",
    "        embedding_matrix[i] = w2v_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_grams = [3, 4, 5]\n",
    "conv_layers = [3, 6, 10]\n",
    "dense_neurons = [15, 30, 60]\n",
    "conv_activations = ['tanh', 'sigmoid','hard_sigmoid', 'relu']\n",
    "regularization = [0, 0.1, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, concatenate, Activation, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "\n",
    "tweet_input = Input(shape=(SENTENCE_LENGTH,), dtype='int32')\n",
    "tweet_encoder = Embedding(NUM, DIM, input_length=SENTENCE_LENGTH,\n",
    "                          weights=[embedding_matrix], trainable=False)(tweet_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "res_arr = []\n",
    "total_combinations = len(conv_grams) * len(conv_layers) * len(conv_activations) * len(regularization) * len(dense_neurons)\n",
    "counter = 1\n",
    "\n",
    "for conv_gram in conv_grams:\n",
    "    conv_arr = np.arange(2, conv_gram + 1)\n",
    "    for conv_layer_number in conv_layers:\n",
    "        for dense_neuron_number in dense_neurons:\n",
    "            for conv_activation in conv_activations:\n",
    "                for reg in regularization:\n",
    "                    branches = []\n",
    "\n",
    "                    if(reg != 0):\n",
    "                        x = Dropout(reg)(tweet_encoder)\n",
    "                    else:\n",
    "                         x = tweet_encoder\n",
    "                    for size in conv_arr:\n",
    "                        for i in range(conv_layer_number):\n",
    "                            branch = Conv1D(filters=1, kernel_size=size.item(), padding='valid', activation=conv_activation)(x)\n",
    "                            branch = GlobalMaxPooling1D()(branch)\n",
    "                            branches.append(branch)\n",
    "\n",
    "                    x = concatenate(branches, axis=1)\n",
    "                    if(reg != 0):\n",
    "                        x = Dropout(reg)(x)\n",
    "                    x = Dense(dense_neuron_number, activation=conv_activation)(x)\n",
    "                    x = Dense(1)(x)\n",
    "                    output = Activation('sigmoid')(x)\n",
    "                    model = Model(inputs=[tweet_input], outputs=[output])\n",
    "                    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[precision, recall, f1])\n",
    "                    model.fit(x_train_seq, y_train, batch_size=32, epochs=3, validation_split=0.25)\n",
    "                    predicted = np.round(model.predict(x_test_seq))\n",
    "                    loss_and_metrics = classification_report(y_test, predicted, output_dict=True)\n",
    "                    res = {\n",
    "                        'ngrams': conv_gram,\n",
    "                        'layers': conv_layer_number,\n",
    "                        'dense': dense_neuron_number,\n",
    "                        'activation': conv_activation,\n",
    "                        'regularization': reg,\n",
    "                        'result': loss_and_metrics \n",
    "                    }\n",
    "                    res_arr.append(res)\n",
    "                    print (str(counter) + ' / ' + str(total_combinations) + ' ready:')\n",
    "                    print(res)\n",
    "                    counter = counter + 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "df = DataFrame(res_arr)\n",
    "columns_to_extract = [ 'precision', 'recall','f1-score']\n",
    "for column in columns_to_extract:\n",
    "    df[column] = df['result'].apply(lambda x: x['weighted avg'][column])\n",
    "df = df.drop(['result'], axis=1)\n",
    "print(df)\n",
    "res_str = df.to_csv(sep=';').replace('.', ',')\n",
    "f = open(\"comparison_cnn.csv\", \"w\")\n",
    "f.write(res_str)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from io import StringIO\n",
    "f = open(\"comparison_cnn.csv\", \"r\")\n",
    "raw_df = f.read().replace(',', '.')\n",
    "df = pandas.read_csv(StringIO(raw_df), sep=';', index_col=False)\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "df['ngrams'] = df['ngrams'].apply(lambda x: str(x))\n",
    "df['dense'] = df['dense'].apply(lambda x: str(x))\n",
    "df['regularization'] = df['regularization'].apply(stringify)\n",
    "print(df)\n",
    "# res_str = df.to_csv(sep=';').replace('.', ',')\n",
    "# f = open(\"comparison.csv\", \"w\")\n",
    "# f.write(res_str)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_activation = df.groupby('activation').mean().reset_index()\n",
    "mean_layers = df.groupby('layers').mean().reset_index()\n",
    "mean_dense = df.groupby('dense').mean().reset_index()\n",
    "mean_ngrams = df.groupby('ngrams').mean().reset_index()\n",
    "mean_regularization = df.groupby('regularization').mean().reset_index()\n",
    "best_activation = df.groupby('activation').max().reset_index()\n",
    "best_layers = df.groupby('layers').max().reset_index()\n",
    "best_dense = df.groupby('dense').max().reset_index()\n",
    "best_ngrams = df.groupby('ngrams').max().reset_index()\n",
    "best_regularization = df.groupby('regularization').mean().reset_index()\n",
    "mean_layers['layers'] = mean_layers['layers'].apply(lambda x: str(x))\n",
    "best_layers['layers'] = best_layers['layers'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.subplots_adjust(hspace=0.8)\n",
    "def plot_grouped(df, group_column, xlabel, pos, ylim=(0.4, 0.83)):\n",
    "    y, x, i = pos\n",
    "    plt.subplot(y, x, i)\n",
    "    plt.ylim(*ylim)\n",
    "    \n",
    "    plt.bar(df[group_column], df['f1-score'])\n",
    "    plt.xlabel(xlabel, fontsize=12)\n",
    "    plt.ylabel('F-мера', fontsize=12)\n",
    "    plt.xticks(rotation=45, fontsize=12)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_grouped(best_activation, 'activation', 'Функция активации', (1,1,1), ylim=(0.69, 0.74))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_grouped(best_layers, 'layers', 'Количество сверточных слоев', (1,1,1), ylim=(0.71, 0.74))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_grouped(best_dense, 'dense', 'Высота полносвязного слоя', (1,1,1), ylim=(0.71,0.74))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_grouped(best_ngrams, 'ngrams', 'Максимальная высота фильтров', (1,1,1), ylim=(0.71, 0.74))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_grouped(best_regularization, 'regularization', 'Регуляризация', (1,1,1), ylim=(0.68, 0.72))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grouped(best_classifiers, 'classifier', 'Классификаторы', (1,1,1),  ylim=(0., 0.83))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diplom",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
