{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#http://datareview.info/article/sovremennyie-metodyi-analiza-tonalnosti-teksta/\n",
    "import pandas\n",
    "import numpy as np\n",
    "positive_raw = pandas.read_csv('positive.csv', sep=';', header=None)\n",
    "negative_raw = pandas.read_csv('negative.csv', sep=';', header=None)\n",
    "print(type(positive_raw[[3,4]]))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_raw = positive_raw.append(negative_raw)\n",
    "all_raw = all_raw[[3, 4]]\n",
    "all_raw.columns = ['text', 'isPositive']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def get_normalized_form(word):\n",
    "    return morph.parse(word)[0].normal_form\n",
    "\n",
    "regex = re.compile(r\"@[\\w]+\")\n",
    "def proceed_string(input_str):\n",
    "    input_str = input_str.replace('\\n', ' ')\n",
    "    input_str = input_str.replace('\\\\n', ' ')\n",
    "    input_str = re.sub(r\"@[\\w]+:\", \"\", input_str)\n",
    "    input_str = re.sub(r\"[^ЁёА-Яа-я\\s]\", \" \", input_str)\n",
    "    input_str = input_str.lower()\n",
    "    words_arr = input_str.split()\n",
    "    words_arr = [get_normalized_form(w) for w in words_arr]\n",
    "    return words_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_raw['text'] = all_raw['text'].map(proceed_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(all_raw['text'], all_raw['isPositive'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7288299, 9988040)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_dim = 200\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "twits_w2v = Word2Vec(size=n_dim, min_count=5)\n",
    "twits_w2v.build_vocab(x_train)\n",
    "twits_w2v.train(x_train, total_examples=len(x_train), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_arr_to_vec(arr, length):\n",
    "    vec = np.zeros(length)\n",
    "    count = 0\n",
    "    for word in arr:\n",
    "        try:\n",
    "            vec += twits_w2v[word]\n",
    "            count += 1.0\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "train_vecs = np.array([word_arr_to_vec(arr, 200) for arr in x_train])\n",
    "train_vecs = scale(train_vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70178586,  2.4893848 ,  1.58166911, ...,  0.66997997,\n",
       "        -0.63002943, -1.24321168],\n",
       "       [ 0.45438809,  0.11922469,  1.36026255, ..., -0.05906654,\n",
       "         0.88639398, -0.06776256],\n",
       "       [ 0.97004538,  0.13852308,  0.2736367 , ..., -0.41439339,\n",
       "         0.12914447, -0.4714326 ],\n",
       "       ...,\n",
       "       [ 0.19405161,  0.39043767,  0.48806454, ...,  1.26018358,\n",
       "         0.86359886, -0.48717387],\n",
       "       [ 0.84344425,  0.27501822, -0.34013677, ..., -0.13926267,\n",
       "         0.44107601,  0.09638735],\n",
       "       [-0.25798638, -0.76816398,  1.04950838, ...,  1.003381  ,\n",
       "        -1.04009795,  1.25157946]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twits_w2v.train(x_test, total_examples=len(x_test), epochs=5)\n",
    "test_vecs = np.array([word_arr_to_vec(arr, 200) for arr in x_test])\n",
    "test_vecs = scale(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.63\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "lr = SGDClassifier(loss='log', penalty='l1')\n",
    "lr.fit(train_vecs, y_train)\n",
    "\n",
    "print ('Test Accuracy: %.2f'%lr.score(test_vecs, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('странный', 0.6741325259208679),\n",
       " ('страшный', 0.6673437356948853),\n",
       " ('хороший', 0.663711428642273),\n",
       " ('ужасный', 0.6586064100265503),\n",
       " ('важный', 0.6497070789337158),\n",
       " ('прекрасный', 0.6259967088699341),\n",
       " ('замечательный', 0.6204522848129272),\n",
       " ('интересный', 0.6160957217216492),\n",
       " ('гавный', 0.613215446472168),\n",
       " ('скучный', 0.6131846904754639)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twits_w2v.most_similar(positive = 'плохой')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
